---
title: "Unit Test Demo"
output: html_document
date: "2023-09-14"
---

```{css, echo=FALSE}
.scroll-100 {
  max-height: 100px;
  overflow-y: auto;
}
```

```{r setup, include=FALSE}
library(tcplfit2)
library(stringr)
library(testthat)
```

# Introduction
The purpose of having unit tests is to ensure future feature development does not affect current analyses. How unit test work is that they compare the results from the codes (usually after some changes made) with previous known results to ensure the output is not changed when it's not supposed to. This vignette will demonstrate how to create and run unit tests, and how failed tests can help detect bug and prevent altering current analyses.  

# Create and Run Unit Tests
Use `fitexp3` as an example. We want to create a unit test for `fitexp3` to make sure if we add more arguments to this function or add more changes to the package, they does not change the current fitting result. The current fitting result for exponential 3 curve with some simulated data is as below.

```{r, example unit test}
data("signatures")
conc=as.numeric(str_split(signatures[1,"conc"],"\\|")[[1]])
resp=as.numeric(str_split(signatures[1,"resp"],"\\|")[[1]])
# use this result as the "expected" result
fitexp3(conc, resp)
```

To create a test for a function, one first need to use `usethis::use_test("<name of the function>")` to create a test file. In that test file contains template codes of `test_that` function from `testthat` package. The following code chunk shows the test created for `fitexp3`. It run the same codes from the code chunk above and compare the fitted values for all model parameters to what we expect to see (result from above). If they are all identical, the test passes. This code chunk can be ran directly and the output would be either you pass or did not pass the test. 

```{r, example unit test exp3}
test_that("fitexp3 works", {
  
  # include the same codes that expect to generate the same results 
  data("signatures")
  conc=as.numeric(str_split(signatures[1,"conc"],"\\|")[[1]])
  resp=as.numeric(str_split(signatures[1,"resp"],"\\|")[[1]])
  
  # compare the actual result with the expected 
  expect_equal(fitexp3(conc, resp)$a, 2.786, tolerance = 1e-3)
  expect_equal(fitexp3(conc, resp)$b, 269.522, tolerance = 1e-3)
  expect_equal(fitexp3(conc, resp)$p, 0.6895, tolerance = 1e-3)
})
```

Repeat the process to create a test file for each function you want to test in a package. We can use `devtools::test()` to run all tests in the package at once. The output will show have many tests are ran and how many passed. For any failed tests, the result will show what is the actual output and what is the expected output.

```{r run all tests}
# make sure you are in the package directory, or specify the path to it
devtools::test()
```

# Failed Test
This section will show you what can cause a unit test to fail. For example, I'm working on adding a new argument to allow users to choose normal as error distribution. The default for this argument would still be t-distribution as is right now, and adding this feature should not change the fitting result if normal error assumption is not used. 

Let say I accidentally hard code function `fitexp3` to use normal for error distribution, then it will return different fitted parameter values and cause the unit test to fail. The code chunk below contains codes with a bug: in the optimization step and where it calculates standard deviations for parameters, I hard coded the error function to be normal. 

**Please note that running the codes in the code chunk below will mask the original `fitexp3` function in `tcplfit2` package. Please reminder to re-load the package if you ran this chunk.** 
```{r hard code exp3, class.source="scroll-100"}
# running this code will mask the function in the original package
# reminder to re-load the package if you ran this chunk 

fitexp3 = function(conc, resp, bidirectional = TRUE, verbose = FALSE, nofit = FALSE, dmin = .3,
                   errfun = "dt4", ...){ 
  ## adding a new errfun argument, defaults to t-distribution

  fenv <- environment()
  #initialize myparams
  pars <- paste0(c("a", "b", "p", "er"))
  sds <- paste0(c("a", "b", "p","er"), "_sd")
  myparams = c("success", "aic", "cov", "rme", "modl", pars, sds, "pars", "sds")

  #returns myparams with appropriate NAs
  if(nofit){
    out = as.list(rep(NA_real_, length(myparams)))
    names(out) = myparams
    out[["success"]] = out[["cov"]] = NA_integer_
    out[["pars"]] = pars
    out[["sds"]] = sds
    return(out)
  }

  #median at each conc, for multi-valued responses
  rmds <- tapply(resp, conc, median)
  #get max response and corresponding conc
  if(!bidirectional) mmed = rmds[which.max(rmds)] else mmed = rmds[which.max(abs(rmds))] #shortened this code
  mmed_conc <- as.numeric(names(mmed)) #fixed this bug

  resp_max <- max(resp)
  resp_min <- min(resp)
  conc_min <- min(conc)
  conc_max <- max(conc)

  er_est <- if ((rmad <- mad(resp)) > 0) log(rmad) else log(1e-16)

  ###--------------------- Fit the Model ----------------------###
  ## Starting parameters for the Model
  a0 = mmed #use largest response with desired directionality
  if(a0 == 0) a0 = .01  #if 0, use a smallish number
  g <- c(a0, # y scale (a)
         conc_max, # x scale (b); curve scaled to highest resp and max conc
         1.2,       # power(p)
         er_est )# logSigma (er)


  ## Generate the bound matrices to constrain the model.
  #                a   b    p    er
  Ui <- matrix(c( 1,   0,   0,   0,
                 -1,   0,   0,   0,
                  0,   1,   0,   0,
                  0,  -1,   0,   0,
                  0,   0,   1,   0,
                  0,   0,  -1,   0),
                byrow = TRUE, nrow = 6, ncol = 4)

  if(!bidirectional){
    bnds <- c(1e-8*abs(a0), -1e8*abs(a0), # a bounds
              1e-2*conc_max, -1e8*conc_max, # b bounds (lower bound avoids overflow at max conc, max power)
              dmin, -8) # p bounds (p > 1, following bmd guidelines)
  } else {
    bnds <- c(-1e8*abs(a0), -1e8*abs(a0), # a bounds
              1e-2*conc_max, -1e8*conc_max, # b bounds (lower bound avoids overflow at max conc, max power)
             dmin, -8) # p bounds (p > 1, following bmd guidelines)
  }

  Ci <- matrix(bnds, nrow = 6, ncol = 1)

  ## Optimize the model
  fit <- try(constrOptim(g,
                          tcplObj,
                          ui = Ui,
                          ci = Ci,
                          mu = 1e-6,
                          method = "Nelder-Mead",
                          control = list(fnscale = -1,
                                         reltol = 1e-10,
                                         maxit = 6000),
                          conc = conc,
                          resp = resp,
                          fname = "exp3",
                          errfun = "dnorm"), ## should be errfun = errfun
              silent = !verbose)


  ## Generate some summary statistics
  if (!is(fit, "try-error")) { # The model fit the data
    if(verbose) cat("Exp3 >>>",fit$counts[1],fit$convergence,"\n")

    success <- 1L
    aic <- 2*length(fit$par) - 2*fit$value # 2*length(fit$par) - 2*fit$value
    mapply(assign,
           c(pars),
           fit$par,
           MoreArgs = list(envir = fenv))

    ## Calculate rmse for gnls
    modl <- exp3(fit$par, conc)
    rme <- sqrt(mean((modl - resp)^2, na.rm = TRUE))

    ## Calculate the sd for the gnls parameters
    fit$cov <- try(solve(-hessian(tcplObj,
                                   fit$par,
                                   conc = conc,
                                   resp = resp,
                                   fname = "exp3",
                                   errfun = "dnorm")), ## should be errfun = errfun
                    silent = !verbose)

    if (!is(fit$cov, "try-error")) { # Could invert gnls Hessian

      cov <- 1L
      diag_sqrt <- suppressWarnings(sqrt(diag(fit$cov)))
      if (any(is.nan(diag_sqrt))) {
        mapply(assign,
               sds,
               NaN,
               MoreArgs = list(envir = fenv))
      } else {
        mapply(assign,
               sds,
               diag_sqrt,
               MoreArgs = list(envir = fenv))
      }

    } else { # Could not invert gnls Hessian

      cov <- 0L
      mapply(assign,
             c(sds),
             NA_real_,
             MoreArgs = list(envir = fenv))

    }

  } else { # Curve did not fit the data

    success <- 0L
    aic <- NA_real_
    cov <- NA_integer_
    rme <- NA_real_
    modl = NA_real_

    mapply(assign,
           c(pars, sds),
           NA_real_,
           MoreArgs = list(envir = fenv))

  }

  return(mget(myparams))

}

```

Run the unit test for `fitexp3` again, and this time it will fail because a bug was introduced by the new changes. The output will show you what the code returned versus what it expect. 

```{r failed test, error=TRUE}
test_that("fitexp3 works", {
  data("signatures")
  conc=as.numeric(str_split(signatures[1,"conc"],"\\|")[[1]])
  resp=as.numeric(str_split(signatures[1,"resp"],"\\|")[[1]])
  
  expect_equal(fitexp3(conc, resp)$a, 2.786, tolerance = 1e-3)
  expect_equal(fitexp3(conc, resp)$b, 269.522, tolerance = 1e-3)
  expect_equal(fitexp3(conc, resp)$p, 0.6895, tolerance = 1e-3)
})
```

Running all tests as we making changes to the package can ensure the current analyses are not unintentionally affected. 




